{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Aprendizaje Supervisado en Inteligencia Artificial","text":"<p>\u00a1Bienvenido! \ud83c\udf89 En este documento encontrar\u00e1s una introducci\u00f3n a los conceptos b\u00e1sicos del aprendizaje supervisado, una de las ramas fundamentales de la Inteligencia Artificial (IA) y del Machine Learning (ML).</p>"},{"location":"#que-es-el-aprendizaje-supervisado","title":"\ud83d\udcd8 \u00bfQu\u00e9 es el Aprendizaje Supervisado?","text":"<p>El aprendizaje supervisado es un tipo de aprendizaje autom\u00e1tico en el que un modelo se entrena utilizando un conjunto de datos etiquetados. Esto significa que cada ejemplo del conjunto de entrenamiento incluye tanto la entrada (X) como la salida deseada (Y).</p> <p>El objetivo del modelo es aprender la relaci\u00f3n entre las entradas y las salidas para poder predecir correctamente la salida de nuevos datos nunca vistos.</p>"},{"location":"#tipos-de-problemas-supervisados","title":"\ud83e\udde0 Tipos de Problemas Supervisados","text":"<ol> <li> <p>Clasificaci\u00f3n:    El modelo aprende a asignar una etiqueta o categor\u00eda a cada ejemplo.    \ud83d\udccd Ejemplo: Clasificar correos como \"spam\" o \"no spam\".</p> </li> <li> <p>Regresi\u00f3n:    El modelo aprende a predecir un valor num\u00e9rico continuo.    \ud83d\udccd Ejemplo: Predecir el precio de una vivienda seg\u00fan sus caracter\u00edsticas.</p> </li> </ol>"},{"location":"#flujo-de-trabajo-del-aprendizaje-supervisado","title":"\u2699\ufe0f Flujo de Trabajo del Aprendizaje Supervisado","text":"<ol> <li>Recolecci\u00f3n de datos: Se obtiene un conjunto de datos con ejemplos representativos del problema.</li> <li>Preprocesamiento: Limpieza, normalizaci\u00f3n y divisi\u00f3n del conjunto de datos (entrenamiento y prueba).</li> <li>Selecci\u00f3n del modelo: Elegir el algoritmo m\u00e1s adecuado (por ejemplo, SVM, \u00c1rboles de decisi\u00f3n, Redes neuronales, etc.).</li> <li>Entrenamiento: El modelo aprende a partir de los datos etiquetados.</li> <li>Evaluaci\u00f3n: Se mide el rendimiento utilizando m\u00e9tricas como precisi\u00f3n, recall o error cuadr\u00e1tico medio.</li> <li>Predicci\u00f3n: Se aplican los conocimientos adquiridos a nuevos datos.</li> </ol>"},{"location":"#ejemplos-de-algoritmos-comunes","title":"\ud83d\udd0d Ejemplos de Algoritmos Comunes","text":"<ul> <li>Regresi\u00f3n lineal</li> <li>K-Nearest Neighbors (KNN)</li> <li>\u00c1rboles de decisi\u00f3n</li> <li>Random Forest</li> <li>M\u00e1quinas de Vectores de Soporte (SVM)</li> <li>Redes neuronales artificiales</li> </ul>"},{"location":"#evaluacion-del-modelo","title":"\ud83d\udcca Evaluaci\u00f3n del Modelo","text":"<p>Para medir la calidad del modelo se utilizan m\u00e9tricas que dependen del tipo de problema:</p> Tipo de problema M\u00e9tricas comunes Clasificaci\u00f3n Exactitud, Precisi\u00f3n, Recall, F1-score Regresi\u00f3n Error absoluto medio (MAE), Error cuadr\u00e1tico medio (MSE), R\u00b2"},{"location":"#consejos-practicos","title":"\ud83d\udca1 Consejos Pr\u00e1cticos","text":"<ul> <li>Siempre divide tus datos en entrenamiento y prueba (por ejemplo, 80% / 20%).</li> <li>Evita el sobreajuste (overfitting): si el modelo aprende demasiado bien los datos de entrenamiento, fallar\u00e1 en los nuevos.</li> <li>Usa validaci\u00f3n cruzada para estimar el rendimiento real del modelo.</li> </ul>"},{"location":"#conclusion","title":"\ud83e\udde9 Conclusi\u00f3n","text":"<p>El aprendizaje supervisado es la base de muchas aplicaciones modernas de IA, desde sistemas de recomendaci\u00f3n hasta diagn\u00f3stico m\u00e9dico. Dominar sus fundamentos te permitir\u00e1 avanzar hacia t\u00e9cnicas m\u00e1s complejas y poderosas.</p> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: 26/10/2025 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/","title":"\ud83e\udd16 Unidad 1. Machine Learning Basado en el An\u00e1lisis de Datos","text":"<p>Esta unidad introduce los conceptos fundamentales del Machine Learning (ML), su flujo de trabajo, las herramientas clave de la biblioteca <code>scikit-learn</code> de Python, y las metodolog\u00edas esenciales para la preparaci\u00f3n, divisi\u00f3n y preprocesamiento de datos.</p>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#11-que-es-el-machine-learning","title":"1.1. \u00bfQu\u00e9 es el Machine Learning?","text":"<p>El Machine Learning (ML) se define como un campo de estudio que utiliza modelos estad\u00edsticos para aprender de los datos. Un aspecto clave es que modelos relativamente simples pueden realizar predicciones complejas.</p>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#definiciones-clave","title":"Definiciones Clave","text":"<ul> <li>Definici\u00f3n temprana (Samuel, 1959): \"Programar computadoras para que aprendan de la experiencia deber\u00eda eliminar la necesidad de gran parte de este esfuerzo de programaci\u00f3n detallado\".</li> <li>Definici\u00f3n moderna (Mitchell, 1997): \"Se dice que un programa de computadora aprende de la experiencia E con respecto a alguna clase de tareas T y una medida de rendimiento P, si su rendimiento en las tareas T, medido por P, mejora con la experiencia E\".</li> <li>Definici\u00f3n matem\u00e1tica (Ej. Regresi\u00f3n Lineal): Un modelo matem\u00e1tico que intenta encontrar la relaci\u00f3n \u00f3ptima entre variables. Por ejemplo, predecir ventas (Target, \\(y\\)) bas\u00e1ndose en gastos de publicidad (Feature, \\(x\\)). El modelo \\(y = wx + b\\) aprende los par\u00e1metros \\(w\\) (peso) y \\(b\\) iterando desde valores arbitrarios (\\(f_1\\)) hasta un valor \u00f3ptimo (\\(f_3\\)) que minimiza el error.</li> </ul>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#ml-y-otros-campos","title":"ML y Otros Campos","text":"<p>El Machine Learning est\u00e1 profundamente interconectado con otros campos: * Es un subcampo de la Inteligencia Artificial. * Deep Learning es un subcampo del Machine Learning. * Se solapa significativamente con Estad\u00edstica, Miner\u00eda de Datos y Reconocimiento de Patrones.</p>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#tipos-de-machine-learning","title":"Tipos de Machine Learning","text":"<p>Seg\u00fan el m\u00e9todo de supervisi\u00f3n, el ML se divide en: 1.  Supervisado: Se proporciona un patr\u00f3n objetivo (datos etiquetados). El cap\u00edtulo se centra en este tipo, que incluye algoritmos como Regresi\u00f3n Lineal, Regresi\u00f3n Log\u00edstica, \u00c1rboles de Decisi\u00f3n, KNN, SVM y Redes Neuronales. 2.  No Supervisado: El patr\u00f3n objetivo debe ser descubierto (datos no etiquetados). Incluye Clustering, PCA y An\u00e1lisis de Asociaci\u00f3n. 3.  Refuerzo: Se aprende mediante la optimizaci\u00f3n de pol\u00edticas (recompensas y castigos).</p>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#flujo-de-trabajo-del-machine-learning","title":"Flujo de Trabajo del Machine Learning","text":"<p>El proceso general para construir un modelo de ML es: 1.  Definici\u00f3n del Problema: Comprender el objetivo de negocio. 2.  Preparaci\u00f3n de Datos: Recolecci\u00f3n de datos brutos (Raw Data) y preprocesamiento. 3.  Machine Learning (Modelado): Se divide la data en conjuntos de Train (Entrenamiento), Validate (Validaci\u00f3n) y Test (Prueba). 4.  Entrenamiento y Evaluaci\u00f3n: Esta fase incluye Ingenier\u00eda de caracter\u00edsticas (Feature engineering), Modelado y optimizaci\u00f3n (entrenar el modelo con los datos), y Evaluaci\u00f3n de rendimiento (Performance metrics). 5.  Aplicaci\u00f3n: Aplicar el modelo en la vida real.</p>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#parametros-vs-hiperparametros","title":"Par\u00e1metros vs. Hiperpar\u00e1metros","text":"<ul> <li>Par\u00e1metros: Se aprenden desde los datos durante el entrenamiento. Contienen el patr\u00f3n de los datos (ej. \\(w\\) y \\(b\\) en regresi\u00f3n lineal, pesos de una red neuronal).</li> <li>Hiperpar\u00e1metros: Se configuran manualmente por el practicante antes del entrenamiento. Se \"afinan\" (tunan) para optimizar el rendimiento (ej. el valor \\(k\\) en KNN, la tasa de aprendizaje, la profundidad m\u00e1xima de un \u00e1rbol).</li> </ul>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#12-biblioteca-python-scikit-learn","title":"1.2. Biblioteca Python scikit-learn","text":"<p><code>scikit-learn</code> es la biblioteca de ML m\u00e1s representativa de Python.</p>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#caracteristicas","title":"Caracter\u00edsticas","text":"<ul> <li>Proporciona una interfaz de biblioteca integrada y unificada.</li> <li>Incluye una amplia variedad de algoritmos de ML, funciones de preprocesamiento y selecci\u00f3n de modelos.</li> <li>Es simple, eficiente y est\u00e1 construida sobre NumPy, SciPy y matplotlib.</li> <li>Es de c\u00f3digo abierto y puede usarse comercialmente.</li> <li>No soporta GPU.</li> </ul>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#mecanismo-de-scikit-learn","title":"Mecanismo de <code>scikit-learn</code>","text":"<p>El flujo de trabajo de la API de <code>scikit-learn</code> es intuitivo y sigue tres pasos: 1.  Instance: Crear una instancia del objeto del modelo (Estimator). 2.  Fit: Entrenar el modelo con los datos. 3.  Predict / transform: Usar el modelo entrenado para hacer predicciones o transformar datos.</p>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#estimator-classifier-y-regressor","title":"Estimator, Classifier y Regressor","text":"<ul> <li><code>Estimator</code>: El objeto base. Aprende de los datos usando el m\u00e9todo <code>.fit()</code> y puede hacer predicciones usando <code>.predict()</code>.</li> <li><code>Classifier</code>: Un estimador para tareas de clasificaci\u00f3n (ej. <code>DecisionTreeClassifier</code>, <code>KNeighborsClassifier</code>).</li> <li><code>Regressor</code>: Un estimador para tareas de regresi\u00f3n (predicci\u00f3n num\u00e9rica) (ej. <code>LinearRegression</code>, <code>KNeighborsRegressor</code>).</li> </ul>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#sintaxis-basica-de-scikit-learn","title":"Sintaxis B\u00e1sica de <code>scikit-learn</code>","text":"<ul> <li>Importar un estimador: <code>from sklearn.linear_model import LinearRegression</code></li> <li>Importar un preprocesador: <code>from sklearn.preprocessing import StandardScaler</code></li> <li>Importar divisi\u00f3n de datos: <code>from sklearn.model_selection import train_test_split</code></li> <li> <p>Importar m\u00e9tricas: <code>from sklearn import metrics</code></p> </li> <li> <p>Instanciar (con hiperpar\u00e1metros): <code>myModel = KNeighborsClassifier(n_neighbors=10)</code></p> </li> <li>Dividir los datos (Hold-out): <code>X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)</code></li> <li>Entrenar el modelo (Supervisado): <code>myModel.fit(X_train, Y_train)</code></li> <li>Hacer predicciones: <code>Y_pred = myModel.predict(X_test)</code></li> <li>Evaluar el rendimiento: <code>metrics.accuracy_score(Y_test, Y_pred)</code></li> <li>Afinar hiperpar\u00e1metros (con Cross-Validation): <code>myGridCV = GridSearchCV(estimator, parameter_grid, cv=5)</code></li> </ul>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#ejemplo-practico-estandarizacion","title":"Ejemplo Pr\u00e1ctico: Estandarizaci\u00f3n","text":"<p>El preprocesamiento, como la estandarizaci\u00f3n, es crucial para mejorar el rendimiento. La estandarizaci\u00f3n (o z-transformation) convierte los datos para que sigan una distribuci\u00f3n normal est\u00e1ndar, usando la f\u00f3rmula \\(z = \\frac{x - m}{\\sigma}\\) (donde \\(m\\) es la media y \\(\\sigma\\) la desviaci\u00f3n est\u00e1ndar).</p> <p>En <code>scikit-learn</code>, se usa <code>StandardScaler</code>: 1.  Importar: <code>from sklearn.preprocessing import StandardScaler</code> 2.  Instanciar: <code>scaler = StandardScaler()</code> 3.  Ajustar (Fit): Se aprende la media (\\(m\\)) y la desviaci\u00f3n (\\(\\sigma\\)) solo de los datos de entrenamiento:     <code>scaler.fit(X_train)</code> 4.  Transformar: Se aplica la transformaci\u00f3n a los datos de entrenamiento y prueba:     <code>X_train = scaler.transform(X_train)</code> <code>X_test = scaler.transform(X_test)</code> 5.  <code>fit_transform</code>: Se pueden combinar los pasos 3 y 4 (solo para <code>X_train</code>):     <code>X_train = scaler.fit_transform(X_train)</code></p> <p>Antes de la estandarizaci\u00f3n, las columnas pueden tener rangos de valores muy diferentes. Despu\u00e9s, todos los valores est\u00e1n centrados alrededor de 0, lo que ayuda a muchos algoritmos a converger mejor.</p>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#modulos-principales-de-scikit-learn","title":"M\u00f3dulos Principales de <code>scikit-learn</code>","text":"M\u00f3dulo Funci\u00f3n Principal Ejemplos <code>sklearn.datasets</code> Cargar datasets de ejemplo. <code>load_iris()</code>, <code>load_breast_cancer()</code> <code>sklearn.preprocessing</code> Preprocesamiento de datos (escalado, codificaci\u00f3n). <code>StandardScaler</code>, <code>LabelEncoder</code>, <code>OneHotEncoder</code> <code>sklearn.model_selection</code> Divisi\u00f3n de datos, validaci\u00f3n y afinado de hiperpar\u00e1metros. <code>train_test_split</code>, <code>GridSearchCV</code>, <code>KFold</code> <code>sklearn.metrics</code> Evaluaci\u00f3n de rendimiento del modelo. <code>accuracy_score</code>, <code>precision_score</code>, <code>recall_score</code>, <code>roc_auc_score</code> <code>sklearn.linear_model</code> Algoritmos lineales. <code>LinearRegression</code>, <code>LogisticRegression</code> <code>sklearn.tree</code> Algoritmos de \u00c1rboles de Decisi\u00f3n. <code>DecisionTreeClassifier</code> <code>sklearn.neighbors</code> Algoritmos de vecinos cercanos. <code>KNeighborsClassifier</code> (K-NN) <code>sklearn.svm</code> Support Vector Machine (M\u00e1quinas de Vectores de Soporte). <code>SVC</code> <code>sklearn.ensemble</code> Algoritmos de Ensamblado (Ensemble). <code>RandomForestClassifier</code>, <code>AdaBoostClassifier</code> <code>sklearn.cluster</code> Algoritmos de clustering (No supervisado). <code>KMeans</code>, <code>DBSCAN</code> <code>sklearn.pipeline</code> Herramienta para encadenar pasos de preprocesamiento y modelado. <code>Pipeline</code>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#13-preparacion-y-division-del-dataset","title":"1.3. Preparaci\u00f3n y Divisi\u00f3n del Dataset","text":"<p>La divisi\u00f3n de datos es fundamental para evaluar un modelo de ML. El conjunto de datos general se divide en un conjunto de entrenamiento y uno de evaluaci\u00f3n (prueba).</p>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#overfitting-sobreajuste-y-generalizacion","title":"Overfitting (Sobreajuste) y Generalizaci\u00f3n","text":"<ul> <li>Generalizaci\u00f3n: Es la capacidad del modelo para predecir con precisi\u00f3n datos nuevos que no ha visto antes.</li> <li>Overfitting: Ocurre cuando un modelo se ajusta demasiado a los datos de entrenamiento, aprendiendo incluso el ruido.</li> <li>Underfitting (Subajuste): Ocurre cuando un modelo es demasiado simple (baja capacidad) y no puede capturar el patr\u00f3n subyacente de los datos.</li> </ul> <p>El Dilema: A medida que aumenta la complejidad (flexibilidad) del modelo: * El error en el conjunto de entrenamiento (Training set) siempre disminuye. * El error en el conjunto de prueba (Test set) disminuye al principio, pero luego comienza a aumentar. El punto donde el error de prueba empieza a subir es donde comienza el overfitting.</p> <p>El conjunto de prueba es esencial para detectar el overfitting y seleccionar un modelo que generalice bien.</p>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#cross-validation-validacion-cruzada","title":"Cross-Validation (Validaci\u00f3n Cruzada)","text":"<p>El conjunto de prueba (Test set) debe usarse \u00a1solo una vez! al final, para la evaluaci\u00f3n final.</p> <p>Para evaluar el modelo durante el entrenamiento (por ejemplo, para afinar hiperpar\u00e1metros), necesitamos una forma de simular un \"conjunto de prueba\" sin tocar el real. Para esto, dividimos el conjunto de entrenamiento (Training Data) en dos partes m\u00e1s peque\u00f1as: un nuevo conjunto de <code>Train</code> y un conjunto de <code>Cross Validate</code> (Validaci\u00f3n).</p> <p>M\u00e9todo: k-Fold Cross-Validation Es el m\u00e9todo m\u00e1s com\u00fan: 1.  Se subdivide el conjunto de entrenamiento (original) en k partes iguales (folds). (Usualmente k=10). 2.  Se itera k veces (rondas). 3.  En cada ronda, se usa 1 fold como conjunto de validaci\u00f3n y los k-1 folds restantes como conjunto de entrenamiento. 4.  Se calcula la m\u00e9trica de rendimiento (ej. accuracy) en cada ronda. 5.  El rendimiento final del modelo es el promedio de las m\u00e9tricas de las k rondas.</p> <p>M\u00e9todo: Leave One Out (LOO) Es un caso extremo de k-Fold donde \\(k\\) es igual al n\u00famero total de muestras. Se entrena con todos los datos menos uno, y se valida con ese \u00fanico dato. Es computacionalmente muy costoso.</p>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#14-preprocesamiento-de-datos","title":"1.4. Preprocesamiento de Datos","text":"<p>Preparar los datos es vital para un buen modelo. Esto incluye la limpieza (manejo de valores at\u00edpicos y faltantes) y la transformaci\u00f3n (escalado y codificaci\u00f3n).</p>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#manejo-de-valores-faltantes-missing-values","title":"Manejo de Valores Faltantes (Missing Values)","text":"<p>Los valores faltantes (identificados en Python como <code>np.nan</code> o <code>NaN</code>) deben ser tratados.</p> <p>1. Identificaci\u00f3n: Se pueden contar usando <code>df.isnull().sum()</code>.</p> <p>2. Eliminaci\u00f3n (con Pandas <code>dropna()</code>): * <code>df.dropna()</code>: Elimina cualquier fila que contenga al menos un <code>NaN</code> (eje por defecto 0). * <code>df.dropna(axis=1)</code>: Elimina cualquier columna que contenga un <code>NaN</code>. * <code>df.dropna(how='all')</code>: Elimina filas/columnas donde todos los valores son <code>NaN</code>. * <code>df.dropna(thresh=N)</code>: Mantiene las filas que tienen al menos <code>N</code> valores no-<code>NaN</code>. * <code>df.dropna(subset=['col_name'])</code>: Elimina filas que tienen <code>NaN</code> espec\u00edficamente en la columna 'col_name'.</p> <p>3. Imputaci\u00f3n (Relleno): Se usa cuando eliminar datos resultar\u00eda en una p\u00e9rdida significativa de informaci\u00f3n. * M\u00e9todos simples: Rellenar con un valor (ej. 'unknown'), la media, la mediana o el valor m\u00e1s frecuente (moda) de la columna. * Con <code>scikit-learn</code> (<code>SimpleImputer</code>): Es el m\u00e9todo preferido.     * <code>from sklearn.impute import SimpleImputer</code>     * <code>impt = SimpleImputer(strategy='mean')</code> (Estrategias: 'mean', 'median', 'most_frequent').     * <code>impt.fit(X_train)</code>: Aprende la media (o mediana/moda) del set de entrenamiento.     * <code>X_train_imputed = impt.transform(X_train)</code>: Aplica la imputaci\u00f3n.     * <code>X_test_imputed = impt.transform(X_test)</code>: Aplica la misma imputaci\u00f3n (con la media de train) al set de prueba.</p>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#manejo-de-datos-categoricos","title":"Manejo de Datos Categ\u00f3ricos","text":"<p>Los algoritmos de ML requieren entradas num\u00e9ricas. Los datos categ\u00f3ricos deben ser convertidos.</p> <p>1. Datos Ordinales (con orden): Ej. Tallas: 'M' &lt; 'L' &lt; 'XL'. Se deben mapear a enteros que respeten ese orden. * <code>size_mapping = {'M': 1, 'L': 2, 'XL': 3}</code> * <code>df['size'] = df['size'].map(size_mapping)</code></p> <p>2. Datos Nominales (sin orden) y Etiquetas de Clase: Ej. Colores: 'red', 'green', 'blue' o Etiquetas: 'setosa', 'versicolor'.</p> <ul> <li> <p>Codificaci\u00f3n de Etiquetas (Label Encoding):     Convierte cada etiqueta \u00fanica en un entero (ej. 'class1': 0, 'class2': 1). Se usa <code>LabelEncoder</code> de <code>scikit-learn</code>.</p> <ul> <li><code>from sklearn.preprocessing import LabelEncoder</code></li> <li><code>enc = LabelEncoder()</code></li> <li><code>y_encoded = enc.fit_transform(df['classlabel'])</code></li> </ul> </li> <li> <p>One-Hot Encoding (para caracter\u00edsticas nominales):     Usar <code>LabelEncoder</code> para caracter\u00edsticas (X) es incorrecto, ya que crea un orden artificial. Se debe usar One-Hot Encoding.     Crea nuevas columnas \"dummy\" (0 o 1) para cada categor\u00eda, indicando presencia (1) o ausencia (0).</p> <ul> <li>M\u00e9todo Pandas: <code>pd.get_dummies(df['species'])</code>.</li> <li>M\u00e9todo <code>scikit-learn</code>: <code>OneHotEncoder</code>. Este m\u00e9todo es preferido en pipelines y a menudo devuelve una matriz dispersa (sparse matrix) para ahorrar memoria, ya que la mayor\u00eda de los valores ser\u00e1n 0.</li> </ul> </li> </ul>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#division-de-datos-estratificada-stratify","title":"Divisi\u00f3n de Datos Estratificada (Stratify)","text":"<p>Al usar <code>train_test_split</code>, si el dataset est\u00e1 desbalanceado (ej. 90% clase A, 10% clase B), una divisi\u00f3n aleatoria simple podr\u00eda resultar en un set de prueba sin muestras de la clase B. * Soluci\u00f3n: Usar el par\u00e1metro <code>stratify=y</code>. * Esto asegura que la proporci\u00f3n de las clases (ej. 90/10) se mantenga id\u00e9ntica tanto en el conjunto de entrenamiento como en el de prueba, reflejando el dataset original.</p>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#topicos-avanzados-tradeoff-de-sesgo-varianza-y-regularizacion","title":"T\u00f3picos Avanzados: Tradeoff de Sesgo-Varianza y Regularizaci\u00f3n","text":"<ul> <li>Tradeoff de Sesgo-Varianza:<ul> <li>Sesgo (Bias): Error por suposiciones incorrectas (Underfitting).</li> <li>Varianza (Variance): Error por sensibilidad excesiva a los datos de entrenamiento (Overfitting).</li> <li>Error Total \\(\\approx\\) Sesgo\u00b2 + Varianza. El objetivo es encontrar la complejidad \u00f3ptima que minimice este error total.</li> </ul> </li> <li>Regularizaci\u00f3n: T\u00e9cnica para prevenir el overfitting en modelos lineales penalizando coeficientes (pesos) grandes.<ul> <li>Ridge (L2): A\u00f1ade una penalizaci\u00f3n \\(\\lambda\\sum{w_j^2}\\). Encoge los pesos, pero no los hace cero.</li> <li>Lasso (L1): A\u00f1ade una penalizaci\u00f3n \\(\\lambda\\sum{|w_j|}\\). Puede forzar que algunos pesos sean exactamente cero, realizando una selecci\u00f3n de caracter\u00edsticas autom\u00e1tica.</li> <li>ElasticNet: Combina penalizaciones L1 y L2.</li> </ul> </li> </ul>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#15-practica-solucion-de-problemas-con-scikit-learn-ej-iris","title":"1.5. Pr\u00e1ctica: Soluci\u00f3n de Problemas con scikit-learn (Ej. Iris)","text":"<p>Esta secci\u00f3n aplica todos los conceptos anteriores en un caso pr\u00e1ctico completo usando el dataset \"Iris\".</p>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#1-entendimiento-del-problema-y-datos-eda","title":"1. Entendimiento del Problema y Datos (EDA)","text":"<ul> <li>Objetivo: Clasificar la especie de una flor Iris (Target).</li> <li>Clases (Target): 3 especies (Setosa, Versicolor, Virginica).</li> <li>Caracter\u00edsticas (Features): <code>sepal_length</code>, <code>sepal_width</code>, <code>petal_length</code>, <code>petal_width</code>.</li> <li>An\u00e1lisis de Datos:<ul> <li>Se cargan los datos y se convierten a un DataFrame de Pandas.</li> <li>Valores Faltantes: Se comprueba con <code>iris.isnull().sum()</code>. No se encontraron.</li> <li>Distribuci\u00f3n de Clases: Se comprueba con <code>iris.groupby('target').size()</code>. Hay 50 muestras de cada clase (33.3% cada una). Es un dataset balanceado.</li> <li>Estad\u00edsticas y Correlaci\u00f3n: <code>iris.describe()</code> y <code>iris.corr()</code>. Se observa que <code>petal_length</code> y <code>petal_width</code> est\u00e1n altamente correlacionados (0.96), sugiriendo un problema de multicolinealidad.</li> <li>Visualizaci\u00f3n: Se usan <code>pairplot</code> y <code>heatmap</code> para confirmar visualmente las relaciones y la alta correlaci\u00f3n.</li> </ul> </li> </ul>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#2-division-y-preparacion-de-datos","title":"2. Divisi\u00f3n y Preparaci\u00f3n de Datos","text":"<ul> <li>Separaci\u00f3n X/y: Se separan las caracter\u00edsticas (X) del objetivo (y).</li> <li>Divisi\u00f3n Train/Test: Se usa <code>train_test_split</code> (ej. 80% train, 20% test).</li> <li>Validaci\u00f3n Cruzada:<ul> <li>Se muestra c\u00f3mo usar <code>KFold</code> (CV est\u00e1ndar) y <code>StratifiedKFold</code> (CV estratificada).</li> <li><code>StratifiedKFold</code> es preferible porque mantiene la distribuci\u00f3n 33/33/33 de las clases en cada fold, asegurando que la validaci\u00f3n sea representativa.</li> </ul> </li> </ul>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#3-seleccion-y-evaluacion-del-modelo","title":"3. Selecci\u00f3n y Evaluaci\u00f3n del Modelo","text":"<ul> <li>Curva de Aprendizaje (<code>Learning Curve</code>):     Se usa para diagnosticar bias vs. variance. Muestra el rendimiento del modelo a medida que ve m\u00e1s datos de entrenamiento.</li> <li>Afinado de Hiperpar\u00e1metros (<code>GridSearchCV</code>):     Se utiliza para encontrar la mejor combinaci\u00f3n de hiperpar\u00e1metros (ej. <code>criterion</code>, <code>max_depth</code> para un <code>DecisionTreeClassifier</code>) probando todas las combinaciones posibles mediante validaci\u00f3n cruzada.</li> </ul>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#4-metricas-de-evaluacion-clasificacion","title":"4. M\u00e9tricas de Evaluaci\u00f3n (Clasificaci\u00f3n)","text":"<p>Una vez que el modelo (<code>GridSearchCV</code>) est\u00e1 entrenado y se hacen predicciones sobre el <code>X_test</code>, se eval\u00faa el rendimiento.</p> <ul> <li> <p>Matriz de Confusi\u00f3n (<code>Confusion Matrix</code>):     Es la base para todas las m\u00e9tricas. Compara los valores reales (True label) con los predichos (Predicted label).</p> <ul> <li>TP (True Positive): Real = 1, Predicho = 1.</li> <li>FN (False Negative): Real = 1, Predicho = 0.</li> <li>FP (False Positive): Real = 0, Predicho = 1.</li> <li>TN (True Negative): Real = 0, Predicho = 0.</li> </ul> </li> <li> <p>M\u00e9tricas Clave:</p> <ul> <li>Accuracy (Exactitud): \\(\\frac{TP + TN}{Total}\\). Proporci\u00f3n de predicciones correctas. (Usar con cuidado en datasets desbalanceados).</li> <li>Precision (Precisi\u00f3n): \\(\\frac{TP}{TP + FP}\\). De los que dijimos que eran positivos, \u00bfcu\u00e1ntos acertamos?.</li> <li>Recall (Sensibilidad o TPR): \\(\\frac{TP}{TP + FN}\\). De todos los positivos reales, \u00bfcu\u00e1ntos encontramos?.</li> <li>F1-Score: La media arm\u00f3nica de Precision y Recall. Es una m\u00e9trica excelente para datasets desbalanceados. \\(F_1 = 2 \\frac{Precision \\times Recall}{Precision + Recall}\\).</li> <li>FPR (Tasa de Falsos Positivos): \\(\\frac{FP}{FP + TN}\\). Proporci\u00f3n de negativos reales que clasificamos incorrectamente como positivos.</li> </ul> </li> <li> <p>Curva ROC y AUC:</p> <ul> <li>Curva ROC: Gr\u00e1fica que muestra el rendimiento de un clasificador en todos los umbrales de clasificaci\u00f3n. Muestra TPR (Eje Y) vs. FPR (Eje X).</li> <li>AUC (Area Under the Curve): El \u00e1rea bajo la curva ROC. Es una m\u00e9trica \u00fanica que resume el rendimiento del modelo.<ul> <li>AUC = 1.0: Clasificador perfecto.</li> <li>AUC = 0.5: Clasificador in\u00fatil (aleatorio).</li> <li>Un AUC de 0.85 o m\u00e1s se considera bueno.</li> </ul> </li> </ul> </li> </ul>"},{"location":"1.%20Machine%20Learning%20para%20el%20an%C3%A1lisis%20de%20datos/#5-prediccion-final","title":"5. Predicci\u00f3n Final","text":"<ul> <li>Se carga el modelo final (el mejor <code>estimator_</code> encontrado por <code>GridSearchCV</code>).</li> <li>Se realizan las predicciones finales sobre el conjunto de prueba (<code>X_test</code>).</li> <li>Los resultados se guardan, por ejemplo, en un archivo CSV.</li> </ul> <p>\ud83d\udcc5 Fecha de creaci\u00f3n: 27/10/2025 \u270d\ufe0f Autor: Fran Garc\u00eda</p>"},{"location":"2.%20Modelos%20de%20de%20Aprendizaje%20Supervisado%20para%20Predicci%C3%B3n%20Num%C3%A9rica/","title":"\ud83e\udd16 Unidad 2. Regresi\u00f3n Lineal para la Inteligencia Artificial","text":"<p>La regresi\u00f3n lineal es uno de los modelos m\u00e1s simples y fundamentales en el campo de la inteligencia artificial y el aprendizaje autom\u00e1tico. A pesar de su simplicidad, proporciona una base s\u00f3lida para entender c\u00f3mo los algoritmos de regresi\u00f3n pueden ser usados para hacer predicciones. En este art\u00edculo exploraremos c\u00f3mo funciona la regresi\u00f3n lineal, sus aplicaciones, y la compararemos con otros modelos de regresi\u00f3n como Ridge y Lasso.</p>"},{"location":"2.%20Modelos%20de%20de%20Aprendizaje%20Supervisado%20para%20Predicci%C3%B3n%20Num%C3%A9rica/#que-es-la-regresion-lineal","title":"\u00bfQu\u00e9 es la Regresi\u00f3n Lineal?","text":"<p>La regresi\u00f3n lineal es un m\u00e9todo estad\u00edstico que intenta modelar la relaci\u00f3n entre una variable dependiente y una o m\u00e1s variables independientes mediante una l\u00ednea recta. La ecuaci\u00f3n que representa una regresi\u00f3n lineal simple tiene la siguiente forma:</p> \\[ y = b_0 + b_1X + \\epsilon \\] <ul> <li>y: Variable dependiente (la que se intenta predecir).</li> <li>X: Variable independiente (el predictor).</li> <li>b_0: Intercepto, valor de y cuando X es cero.</li> <li>b_1: Coeficiente que representa la pendiente de la l\u00ednea.</li> <li>\\(\\epsilon\\): Error o ruido, la diferencia entre la predicci\u00f3n y el valor real.</li> </ul> <p>La regresi\u00f3n lineal se utiliza principalmente para problemas de predicci\u00f3n num\u00e9rica, como el precio de una vivienda, el rendimiento de una acci\u00f3n o cualquier otra situaci\u00f3n en la que exista una relaci\u00f3n lineal entre las variables.</p>"},{"location":"2.%20Modelos%20de%20de%20Aprendizaje%20Supervisado%20para%20Predicci%C3%B3n%20Num%C3%A9rica/#aplicaciones-de-la-regresion-lineal","title":"Aplicaciones de la Regresi\u00f3n Lineal","text":"<p>La regresi\u00f3n lineal es ampliamente utilizada en una variedad de aplicaciones, como:</p> <ul> <li> <p>Econom\u00eda: Predicci\u00f3n de precios de bienes y servicios. Por ejemplo, podemos usar la regresi\u00f3n lineal para modelar la relaci\u00f3n entre la inflaci\u00f3n y el precio de los alimentos. En este caso, la regresi\u00f3n lineal simple puede ser suficiente si se trata de una relaci\u00f3n clara y directa. Ejemplo en Python</p> </li> <li> <p>Finanzas: Estimaci\u00f3n del rendimiento de acciones o bonos. La regresi\u00f3n lineal puede ayudar a estimar c\u00f3mo factores como las tasas de inter\u00e9s y el crecimiento econ\u00f3mico afectan los precios de las acciones. Si existen muchas variables correlacionadas, Ridge Regression ser\u00eda una mejor opci\u00f3n para estabilizar el modelo y evitar el sobreajuste. Ejemplo en Python</p> </li> <li> <p>Salud: Modelado de la relaci\u00f3n entre la dosis de un medicamento y la respuesta del paciente. En este \u00e1mbito, se podr\u00eda usar la regresi\u00f3n lineal para entender c\u00f3mo var\u00eda la presi\u00f3n sangu\u00ednea en respuesta a diferentes dosis de un medicamento. Si existen m\u00faltiples factores (como edad, peso, y otras condiciones de salud), Lasso Regression podr\u00eda ayudar a identificar cu\u00e1les son las caracter\u00edsticas m\u00e1s relevantes. Ejemplo en Python</p> </li> <li> <p>Marketing: Determinaci\u00f3n de la relaci\u00f3n entre el gasto publicitario y las ventas. La regresi\u00f3n lineal se utiliza para estimar el impacto de diferentes estrategias publicitarias en las ventas. Si existen muchas campa\u00f1as publicitarias y se necesita identificar cu\u00e1les son las m\u00e1s efectivas, Lasso Regression podr\u00eda ayudar a eliminar las menos significativas y reducir la complejidad del modelo. </p> </li> <li> <p>Educaci\u00f3n: Predicci\u00f3n de calificaciones de estudiantes en funci\u00f3n de variables como el tiempo de estudio y la asistencia. Si el objetivo es identificar los factores que tienen mayor influencia en el rendimiento acad\u00e9mico, Lasso Regression ser\u00eda \u00fatil para seleccionar solo las caracter\u00edsticas m\u00e1s relevantes, como participaci\u00f3n en clase, tiempo de estudio, o participaci\u00f3n en actividades extracurriculares. </p> </li> <li> <p>Inmobiliaria: Predicci\u00f3n del valor de una propiedad con base en caracter\u00edsticas como la ubicaci\u00f3n, el tama\u00f1o y el n\u00famero de habitaciones. En este contexto, Ridge Regression puede ser \u00fatil para manejar la multicolinealidad, ya que caracter\u00edsticas como la ubicaci\u00f3n y el tama\u00f1o de una propiedad suelen estar correlacionadas. Ridge ayuda a estabilizar los coeficientes y mejorar la capacidad predictiva del modelo. </p> </li> <li> <p>Agricultura: Estimaci\u00f3n del rendimiento de cultivos en funci\u00f3n de factores como el clima, la cantidad de fertilizante y el tipo de suelo. Ridge Regression es adecuada cuando hay m\u00faltiples factores que pueden estar correlacionados, como la temperatura y la precipitaci\u00f3n. Esto ayuda a manejar mejor la multicolinealidad y a mejorar la generalizaci\u00f3n del modelo. Ejemplo en Python</p> </li> </ul>"},{"location":"2.%20Modelos%20de%20de%20Aprendizaje%20Supervisado%20para%20Predicci%C3%B3n%20Num%C3%A9rica/#que-tecnica-es-mas-apropiada","title":"\u00bfQu\u00e9 t\u00e9cnica es m\u00e1s apropiada?","text":"<ul> <li>Econom\u00eda y Finanzas: En estos campos, la regresi\u00f3n lineal puede ser \u00fatil cuando se trata de problemas simples, como la predicci\u00f3n de precios basada en una o dos caracter\u00edsticas. Sin embargo, si hay muchas variables que est\u00e1n altamente correlacionadas, Ridge Regression ser\u00eda m\u00e1s apropiada para evitar el sobreajuste. Ejemplo en Python</li> <li>Salud: Para el modelado de la relaci\u00f3n entre la dosis de un medicamento y la respuesta del paciente, Lasso Regression ser\u00eda adecuada si hay muchas caracter\u00edsticas potenciales, ya que podr\u00eda simplificar el modelo eliminando caracter\u00edsticas irrelevantes. Ejemplo en Python</li> <li>Marketing: Si hay muchas variables de marketing, como diferentes tipos de publicidad, Lasso Regression puede ayudar a identificar cu\u00e1les de ellas son las m\u00e1s importantes, eliminando las menos significativas. </li> <li>Inmobiliaria: En el caso de la predicci\u00f3n de precios de propiedades, Ridge Regression puede ser \u00fatil para manejar la multicolinealidad, ya que a menudo las caracter\u00edsticas como ubicaci\u00f3n, tama\u00f1o y tipo de propiedad est\u00e1n correlacionadas. </li> <li>Educaci\u00f3n: Si queremos predecir las calificaciones de los estudiantes y hay muchas caracter\u00edsticas (como el historial acad\u00e9mico, asistencia, participaci\u00f3n en clase, etc.), Lasso ser\u00eda \u00fatil para identificar las variables m\u00e1s relevantes y eliminar las menos importantes. </li> <li>Agricultura: Para la estimaci\u00f3n del rendimiento de cultivos, Ridge Regression ser\u00eda adecuada si existen m\u00faltiples factores correlacionados, ya que permite manejar mejor la multicolinealidad. Ejemplo en Python</li> </ul>"},{"location":"2.%20Modelos%20de%20de%20Aprendizaje%20Supervisado%20para%20Predicci%C3%B3n%20Num%C3%A9rica/#limitaciones-de-la-regresion-lineal","title":"Limitaciones de la Regresi\u00f3n Lineal","text":"<p>Aunque la regresi\u00f3n lineal es f\u00e1cil de entender y usar, presenta algunas limitaciones importantes que deben tenerse en cuenta al aplicar este modelo:</p> <ul> <li> <p>Supone una relaci\u00f3n lineal: La regresi\u00f3n lineal solo puede modelar relaciones lineales entre las variables. Si la relaci\u00f3n es no lineal, el modelo tendr\u00e1 un rendimiento pobre. Esto implica que, si los datos muestran una relaci\u00f3n m\u00e1s compleja (por ejemplo, cuadr\u00e1tica o exponencial), la regresi\u00f3n lineal no podr\u00e1 capturar dicha complejidad, resultando en predicciones inexactas. En estos casos, ser\u00eda mejor utilizar modelos que puedan capturar la no linealidad, como la regresi\u00f3n polin\u00f3mica o t\u00e9cnicas m\u00e1s avanzadas como redes neuronales.</p> </li> <li> <p>Sensibilidad a los outliers: La presencia de valores at\u00edpicos puede afectar significativamente el ajuste de la l\u00ednea, ya que la regresi\u00f3n lineal minimiza la suma de los errores al cuadrado. Los outliers, al tener errores m\u00e1s grandes, influyen desproporcionadamente en la l\u00ednea de ajuste, lo cual puede distorsionar el modelo. Para mitigar este problema, se pueden utilizar t\u00e9cnicas como la detecci\u00f3n y eliminaci\u00f3n de outliers, o emplear m\u00e9todos de regresi\u00f3n robusta que minimicen el impacto de estos valores extremos.</p> </li> <li> <p>Multicolinealidad: Cuando las variables independientes est\u00e1n altamente correlacionadas, el modelo puede producir resultados inestables. La multicolinealidad genera problemas en la estimaci\u00f3n de los coeficientes, haciendo que sean muy sensibles a peque\u00f1as variaciones en los datos y, por lo tanto, menos interpretables. Esto puede llevar a una disminuci\u00f3n en la precisi\u00f3n de las predicciones y a problemas en la generalizaci\u00f3n del modelo. En estos casos, se recomienda usar t\u00e9cnicas de regularizaci\u00f3n, como Ridge Regression, que penaliza los coeficientes grandes y ayuda a reducir los efectos de la multicolinealidad, estabilizando el modelo.</p> </li> </ul>"},{"location":"2.%20Modelos%20de%20de%20Aprendizaje%20Supervisado%20para%20Predicci%C3%B3n%20Num%C3%A9rica/#ridge-regression-y-lasso-regression","title":"Ridge Regression y Lasso Regression","text":"<p>Para superar algunas de las limitaciones de la regresi\u00f3n lineal est\u00e1ndar, se han desarrollado t\u00e9cnicas de regularizaci\u00f3n como Ridge y Lasso. Ambas t\u00e9cnicas son versiones modificadas de la regresi\u00f3n lineal que incluyen un t\u00e9rmino de penalizaci\u00f3n para mejorar el rendimiento y evitar el sobreajuste.</p>"},{"location":"2.%20Modelos%20de%20de%20Aprendizaje%20Supervisado%20para%20Predicci%C3%B3n%20Num%C3%A9rica/#ridge-regression","title":"Ridge Regression","text":"<p>Ridge Regression, tambi\u00e9n conocida como regresi\u00f3n de cresta, a\u00f1ade un t\u00e9rmino de regularizaci\u00f3n L2 a la funci\u00f3n de p\u00e9rdida. Esto significa que el modelo penaliza los coeficientes grandes, haciendo que los valores de los par\u00e1metros sean m\u00e1s peque\u00f1os y estables. La ecuaci\u00f3n para Ridge es:</p> \\[ J(  heta) = \\sum (y_i - \\hat{y_i})^2 + \\lambda \\sum     heta_j^2 \\] <ul> <li>\\lambda: Par\u00e1metro de regularizaci\u00f3n que controla la cantidad de penalizaci\u00f3n.</li> <li>**    \\(heta_j\\)**: Coeficientes del modelo.</li> </ul> <p>El t\u00e9rmino de penalizaci\u00f3n ayuda a reducir la complejidad del modelo, lo cual resulta \u00fatil especialmente cuando existen m\u00faltiples variables independientes correlacionadas (multicolinealidad).</p>"},{"location":"2.%20Modelos%20de%20de%20Aprendizaje%20Supervisado%20para%20Predicci%C3%B3n%20Num%C3%A9rica/#ventajas-de-ridge-regression","title":"Ventajas de Ridge Regression","text":"<ul> <li>Reducci\u00f3n del sobreajuste: Ridge ayuda a reducir el riesgo de sobreajuste al penalizar los coeficientes grandes.</li> <li>Mejora la estabilidad: Especialmente en presencia de multicolinealidad, el modelo Ridge tiende a ser m\u00e1s estable.</li> </ul>"},{"location":"2.%20Modelos%20de%20de%20Aprendizaje%20Supervisado%20para%20Predicci%C3%B3n%20Num%C3%A9rica/#lasso-regression","title":"Lasso Regression","text":"<p>Lasso Regression a\u00f1ade un t\u00e9rmino de regularizaci\u00f3n L1 a la funci\u00f3n de p\u00e9rdida. Este t\u00e9rmino tiene la capacidad de hacer que algunos coeficientes sean exactamente cero, eliminando efectivamente ciertas caracter\u00edsticas del modelo. La ecuaci\u00f3n de Lasso es:</p> \\[ J(  heta) = \\sum (y_i - \\hat{y_i})^2 + \\lambda \\sum |   heta_j| \\] <ul> <li>\\(\\lambda\\): Par\u00e1metro de regularizaci\u00f3n que controla la penalizaci\u00f3n.</li> </ul> <p>Lasso es \u00fatil no solo para reducir el sobreajuste, sino tambi\u00e9n para la selecci\u00f3n de caracter\u00edsticas, ya que elimina autom\u00e1ticamente aquellas que no son \u00fatiles para la predicci\u00f3n.</p>"},{"location":"2.%20Modelos%20de%20de%20Aprendizaje%20Supervisado%20para%20Predicci%C3%B3n%20Num%C3%A9rica/#ventajas-de-lasso-regression","title":"Ventajas de Lasso Regression","text":"<ul> <li>Selecci\u00f3n de caracter\u00edsticas: Lasso simplifica el modelo seleccionando solo las caracter\u00edsticas m\u00e1s relevantes.</li> <li>Reducci\u00f3n del sobreajuste: Similar a Ridge, Lasso ayuda a evitar el sobreajuste del modelo.</li> </ul>"},{"location":"2.%20Modelos%20de%20de%20Aprendizaje%20Supervisado%20para%20Predicci%C3%B3n%20Num%C3%A9rica/#comparacion-entre-regresion-lineal-ridge-y-lasso","title":"Comparaci\u00f3n entre Regresi\u00f3n Lineal, Ridge y Lasso","text":"Caracter\u00edstica Regresi\u00f3n Lineal Ridge Regression Lasso Regression Regularizaci\u00f3n No L2 L1 Penalizaci\u00f3n Ninguna Penaliza coeficientes grandes Algunos coeficientes se hacen cero Sobreajuste Alta posibilidad Baja Baja Multicolinealidad Problemas con multicolinealidad Mejor manejo Mejor manejo Selecci\u00f3n de caracter\u00edsticas No No S\u00ed <ul> <li>Regresi\u00f3n Lineal: Ideal para problemas simples y cuando existe una relaci\u00f3n lineal clara entre las variables. Sin embargo, es propensa al sobreajuste si no se maneja adecuadamente.</li> <li>Ridge Regression: \u00datil cuando existe multicolinealidad, ya que la regularizaci\u00f3n L2 ayuda a estabilizar el modelo. No elimina caracter\u00edsticas, pero hace que los coeficientes sean m\u00e1s peque\u00f1os.</li> <li>Lasso Regression: \u00datil para la selecci\u00f3n de caracter\u00edsticas, ya que fuerza algunos coeficientes a ser exactamente cero. Esto resulta en un modelo m\u00e1s sencillo y f\u00e1cil de interpretar.</li> </ul>"},{"location":"2.%20Modelos%20de%20de%20Aprendizaje%20Supervisado%20para%20Predicci%C3%B3n%20Num%C3%A9rica/#conclusiones","title":"Conclusiones","text":"<p>La regresi\u00f3n lineal es una excelente herramienta para comenzar a entender los modelos de regresi\u00f3n. Sin embargo, cuando nos enfrentamos a datos m\u00e1s complejos, con m\u00faltiples caracter\u00edsticas y posibles problemas de sobreajuste, Ridge y Lasso se presentan como mejores alternativas. Estos modelos ayudan a mejorar la capacidad de generalizaci\u00f3n del modelo y a reducir la complejidad, haciendo que la predicci\u00f3n sea m\u00e1s precisa y confiable.</p> <p>La elecci\u00f3n entre la regresi\u00f3n lineal, Ridge y Lasso depender\u00e1 de la naturaleza de los datos y los objetivos del an\u00e1lisis. Si se desea simplicidad y no hay riesgo de multicolinealidad, la regresi\u00f3n lineal puede ser suficiente. Si el modelo tiende a sobreajustarse o hay muchas caracter\u00edsticas correlacionadas, Ridge y Lasso son opciones a considerar, siendo Lasso ideal si se desea simplificar el modelo eliminando caracter\u00edsticas irrelevantes.</p>"},{"location":"2.%20Modelos%20de%20de%20Aprendizaje%20Supervisado%20para%20Predicci%C3%B3n%20Num%C3%A9rica/#ejemplos-adicionales-de-uso","title":"Ejemplos Adicionales de Uso","text":"<ul> <li>Predicci\u00f3n de Ventas Minoristas: En un negocio minorista donde existen m\u00faltiples caracter\u00edsticas que afectan las ventas (promociones, temporadas, clima, ubicaci\u00f3n), Ridge Regression ser\u00eda \u00fatil para manejar la posible multicolinealidad entre estas caracter\u00edsticas. Ejemplo en Python</li> <li>Modelado de la Demanda Energ\u00e9tica: En la predicci\u00f3n del consumo de energ\u00eda el\u00e9ctrica, que depende de variables como temperatura, hora del d\u00eda, y tipo de d\u00eda (laboral o festivo), Ridge podr\u00eda ayudar a manejar la complejidad y multicolinealidad. </li> <li>An\u00e1lisis de Sentimientos: Al predecir la polaridad de una opini\u00f3n (positiva o negativa) en base a muchas palabras o frases, Lasso Regression ser\u00eda ideal para seleccionar las palabras m\u00e1s relevantes y reducir la dimensionalidad. </li> <li>Predicci\u00f3n de Costos de Seguros M\u00e9dicos: Para estimar los costos de seguros m\u00e9dicos en funci\u00f3n de caracter\u00edsticas como edad, estado de salud, h\u00e1bitos de vida y ubicaci\u00f3n geogr\u00e1fica, Lasso podr\u00eda ayudar a eliminar caracter\u00edsticas redundantes, haciendo el modelo m\u00e1s interpretable. </li> <li>Optimizaci\u00f3n de Cadenas de Suministro: Para predecir el tiempo de entrega de productos considerando m\u00faltiples variables (tr\u00e1fico, distancia, clima, inventario), Ridge Regression puede ser \u00fatil para manejar la correlaci\u00f3n entre factores como tr\u00e1fico y distancia. </li> <li>Reconocimiento de Actividad Humana: En la clasificaci\u00f3n de actividades humanas usando sensores port\u00e1tiles (como aceler\u00f3metros y giroscopios), Lasso podr\u00eda ayudar a identificar cu\u00e1les de las se\u00f1ales del sensor son m\u00e1s importantes para diferenciar entre actividades como caminar, correr o estar de pie. Ejemplo en Python</li> </ul>"}]}